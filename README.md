# LLM Compiler Error Judge

Este projeto implementa um sistema de avalia√ß√£o autom√°tica para mensagens de erro de compiladores. O objetivo √© determinar se uma mensagem de erro gerada por um "compilador de estudante" √© semanticamente equivalente √† mensagem de erro de refer√™ncia (*gold standard*), mesmo que a fraseologia seja diferente.

O sistema utiliza LLMs locais (via **Ollama**) e t√©cnicas de NLP para atuar como juiz, classificando pares de erros como equivalentes (`True`) ou distintos (`False`).

## üèÜ Melhores Resultados (Destaque)

A arquitetura vencedora utilizou execu√ß√£o paralela com o modelo **Qwen 2.5 3B**.

**Performance de Execu√ß√£o:**
- **Script:** `2_judge_pairs_parallel.py`
- **Tempo Total (754 pares):** ~107s
- **Lat√™ncia M√©dia:** ~142ms por par
- **Throughput:** Processamento altamente eficiente via *Threading*.

**M√©tricas de Classifica√ß√£o:**
- **Acur√°cia:** 85.9%
- **F1-Score (Classe True):** 0.849
- **F1-Score (Classe False):** 0.868

---

## üß™ Experimentos e Abordagens

O projeto explora quatro estrat√©gias diferentes para resolver o problema de verifica√ß√£o de equival√™ncia de erros:

### 1. Abordagem Paralela (Vencedora)
- **Arquivo:** [`2_judge_pairs_parallel.py`](2_judge_pairs_parallel.py)
- **L√≥gica:** Utiliza `ThreadPoolExecutor` para enviar m√∫ltiplas requisi√ß√µes simult√¢neas ao servidor do Ollama.
- **Vantagem:** Maximiza o uso da GPU e reduz drasticamente o tempo ocioso do Python esperando I/O. Foi a abordagem mais r√°pida e est√°vel.

### 2. Abordagem em Lote (Batched)
- **Arquivos:** [`2_judge_pairs_batched.py`](2_judge_pairs_batched.py) e [`2_judge_pairs_batched_v2.py`](2_judge_pairs_batched_v2.py)
- **L√≥gica:** Agrupa m√∫ltiplos pares (ex: 8 ou 16) em um √∫nico prompt gigante e pede ao LLM para retornar um JSON com as respostas.
- **Desafio:** Embora reduza o overhead de HTTP, o modelo √†s vezes falha em formatar o JSON corretamente ou perde a aten√ß√£o em contextos longos.

### 3. Abordagem Sequencial (Baseline)
- **Arquivo:** [`2_judge_pairs.py`](2_judge_pairs.py)
- **L√≥gica:** Itera sobre o dataset um por um, enviando uma requisi√ß√£o por vez.
- **Uso:** Serve como linha de base para medir o ganho de velocidade das outras abordagens. √â robusta, mas lenta.

### 4. Abordagem via Embeddings (Sem LLM)
- **Arquivo:** [`sentence_transform.py`](sentence_transform.py)
- **L√≥gica:** Utiliza `SentenceTransformers` (ex: `all-MiniLM-L6-v2`) para gerar vetores num√©ricos das frases e calcula a Similaridade de Cosseno.
- **Vantagem:** Extremamente r√°pida (milissegundos).
- **Desvantagem:** Tende a ter menor acur√°cia em distin√ß√µes t√©cnicas sutis (ex: confundir "EOF" com "EOL") que o LLM consegue captar via *Few-Shot Prompting*.

---

## üìÇ Estrutura do Pipeline

1.  **Constru√ß√£o do Dataset (`0_build_gold.py`)**:
    Extrai casos de teste de arquivos YAML dentro de um zip (`testslogcomp.zip`) para criar o arquivo `gold.jsonl`.

2.  **Gera√ß√£o de Dados Sint√©ticos (`1_generate_synthetic.py`)**:
    Usa um LLM para criar varia√ß√µes das mensagens de erro:
    - **Pares Positivos:** O LLM parafraseia o erro original (simulando um aluno).
    - **Pares Negativos:** O script mistura erros aleat√≥rios de outros testes.
    - Sa√≠da: `synthetic.jsonl`.

3.  **Julgamento (`2_judge_*.py`)**:
    Executa uma das estrat√©gias de julgamento descritas acima. Gera o arquivo `judgments.jsonl`.

4.  **Avalia√ß√£o (`3_eval_judge.py`)**:
    Compara as previs√µes do modelo com os r√≥tulos reais, gerando Matriz de Confus√£o, Acur√°cia, Precis√£o, Recall e F1.

## üõ†Ô∏è Pr√©-requisitos e Instala√ß√£o

1.  **Python 3.8+**
2.  **Ollama** instalado e rodando localmente.
3.  **Modelo Qwen**:
    ```bash
    ollama pull qwen2.5:3b-instruct
    ```
4.  **Depend√™ncias Python**:
    ```bash
    pip install requests pyyaml numpy scikit-learn sentence-transformers
    ```

## üöÄ Como Executar (Reproduzindo o Melhor Resultado)

1.  **Gerar os dados:**
    ```bash
    python 0_build_gold.py
    python 1_generate_synthetic.py
    ```

2.  **Rodar o Juiz Paralelo:**
    ```bash
    python 2_judge_pairs_parallel.py
    ```

3.  **Verificar M√©tricas:**
    ```bash
    python 3_eval_judge.py
    ```

## üß† Engenharia de Prompt

Os prompts est√£o centralizados em [`prompts.py`](prompts.py). Utilizamos **Few-Shot Prompting**, fornecendo ao modelo exemplos de julgamentos corretos (ex: explicando que "token" e "s√≠mbolo" s√£o sin√¥nimos, mas "EOF" e "EOL" s√£o diferentes) antes de pedir a classifica√ß√£o atual.